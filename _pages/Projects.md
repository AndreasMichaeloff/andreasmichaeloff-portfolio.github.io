---
permalink: /projects/
title: "Projects"
---

- **Project 1.** Creation of Data Pipeline within AWS Sagemaker

Utilizing Google Big Query datasets and Python's Pandas and Polars packages, I made a data pipeline that converted large data sets into optimized Parquet files that reduced cell processing time. I made use of Polar's LazyFrame eature which allowed me to efficiently process and maniulate large datasets by deffering computations until collecting the final end-result. The final result was a parquet file that was a combination of an excel file and multiple BigQuery data sets.

<div style="position: relative; width: 100%; height: 0; padding-top: 75.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https://www.canva.com/design/DAGteKsDZYA/0jtpPTpshZqG-__PNTcjsA/view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGteKsDZYA&#x2F;0jtpPTpshZqG-__PNTcjsA&#x2F;view?utm_content=DAGteKsDZYA&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener">Google Big Query Tables</a> by Andreas Michaeloff

I then took the created Parquet file and turned it into Hyper files for use in Tableau.

- **Project 2. ** Tool to guide customer decision-making process (Trane Technologies)
